{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08c172ad-fcf5-4a9b-95a0-edcc33dbc20e",
   "metadata": {},
   "source": [
    "# Lab 5 Sieci Neuronowe w Automatyce -  Q-learning\n",
    "\n",
    "Celem ćwiczenia jest implementacja algorytmu DQN, który nauczy się sterować odwróconym wahadłem utrzymując je w górze.\n",
    "\n",
    "Punktacja: \n",
    "* 8 pkt. działający kod (wahadło utrzymane w górze powyżej 200 kroków)\n",
    "* 2 pkt. odpowiedzi na pytania\n",
    "\n",
    "Imię i nazwisko: ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d8df9b-b34a-4386-96d5-9ca3cf5565a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# doinstalowujemy potrzebne biblioteki\n",
    "!pip install gymnasium imageio ffmpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660f74c1-d35e-49ba-8f95-65fb76b9f617",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "from gymnasium.wrappers import RecordVideo\n",
    "from gymnasium.wrappers import TimeLimit\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import numpy.testing as npt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb5054d-d263-4358-ba3b-d189b635b00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tworzymy środowisko\n",
    "env = gym.make('CartPole-v1', render_mode=\"rgb_array\")\n",
    "observation, _ = env.reset()\n",
    "print(observation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50c7721-46cf-48c6-a118-4df9c4deb099",
   "metadata": {},
   "source": [
    "#### Obserwacje (stan)\n",
    "Obserwacje to:\n",
    "- $x$ - pozycja wózka\n",
    "- $\\dot{x}$ - prędkość wózka\n",
    "- $\\theta$ - kąt nachylenia wahadła\n",
    "- $\\dot{\\theta}$ - prędkość kątowa wahadła"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768df5a1-266a-4459-af97-79914c00f694",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(env.render());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac98d7f8-7058-49cd-a614-7bca0b3b6382",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a0e0b7-561c-44df-8c99-c7816f05f083",
   "metadata": {},
   "source": [
    "#### Akcje\n",
    "Mamy dwie możliwe akcje:\n",
    "* 0 - przesuń wózek w lewo\n",
    "* 1 - przesuń wózek w prawo\n",
    "\n",
    "#### Nagrody\n",
    "Za każdy krok, póki wahadło nie spadnie, dostajemy nagrodę 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9487680d-149e-43d0-8f5b-f73fad41e5df",
   "metadata": {},
   "source": [
    "### 1. Losowy agent\n",
    "Zaimplementować agenta wykonującego losowe akcje. Uruchomić 5 epizodów. Dla każdego epizodu policzyć skumuowane nagrody i wyświetlić. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27affe3-9986-4c3a-a9ee-e42b3b2c04eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards_list = []\n",
    "for i in range(5):\n",
    "    observation, info = env.reset()\n",
    "    done = False\n",
    "    rewards = 0\n",
    "    while not done:\n",
    "        action = #TODO wybrać losową akcję\n",
    "        observation, reward, terminated, truncated, info = #TODO wykonać akcję\n",
    "        done = terminated or truncated\n",
    "        #TODO: dodać otrzymaną nagrodę do skumulowanej angrody\n",
    "    env.close()\n",
    "    #TODO: dodać skumulowane nagrody do listy nagród"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ce5499-dd56-4e54-9a36-c416e4805ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1234a94-1791-4ed9-a2b7-44a8da335c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tworzymy środowisko i ustawiamy maksymalną liczbę kroków\n",
    "max_steps = 300\n",
    "env = gym.make('CartPole-v1', render_mode=\"rgb_array\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1108c04-8663-43dd-b0be-b7fca4247673",
   "metadata": {},
   "source": [
    "#### Parametry\n",
    "* state_size - rozmiar przestrzeni stanu środowiska\n",
    "* buffer_size - rozmiar bufora\n",
    "* epsilon - początkowa wartość $\\epsilon$ do stosowania polityki $\\epsilon$ zachłannej\n",
    "* epsilon_decay - liczba przez którą mnożymy epsilon po każdym epizodzie\n",
    "* epsilon_min - ale epsilon nie powinien spadać poniżej epsilon_min\n",
    "* gamma - współczynnik dyskontujący\n",
    "* batch_size - rozmiar batcha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94542ebe-efdd-4fc1-9472-b36103cf6621",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_size = 4\n",
    "epsilon = 1\n",
    "epsilon_min = 0.01\n",
    "epsilon_decay = 0.99\n",
    "gamma = 0.95\n",
    "batch_size = 32\n",
    "buffer_size = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59f8422-e476-4afc-ae04-7ae4a268130e",
   "metadata": {},
   "source": [
    "#### 2. Tworzenie DQN\n",
    "Zaimplementuj funkcję tworzącą sieć aproksymującą Q - wartości. Zastosuj:\n",
    "* Dwie warstwy ukryte o 32 neuronach i funkcji aktywacji relu\n",
    "* Liniową funkcję aktywacji w warstwie wyjściowej\n",
    "* Optymalizator Adam i błąd średniokwadratowy jako funkcję straty\n",
    "* Liczba wejść sieci odpowiada rozmiarowi stanu środowiska\n",
    "* Liczba wyjść sieci odpowiada liczbie akcji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8e04ad-af32-4eec-9cb8-fec89766aec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_network():\n",
    "    model = tf.keras.Sequential()\n",
    "    #TODO dodać do sieci warstwy\n",
    "    model.compile(#TODO parametry)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee22c766-eb93-40e5-afb8-2835afd5ccde",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = make_network()\n",
    "state = np.array([[-0.005, 0.001, 0.0001, 0.0001]])\n",
    "q_values = model.predict(state, verbose=0)\n",
    "print(q_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d3eeb4-7ea8-4a05-99d4-f368cb740a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(q_values.shape == (1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444d57dd-b43a-47e7-b11d-f3e66b88aa90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wskazówka do 3\n",
    "q_values.argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e3ed3f-9d2e-4740-b9c8-c2dfcd0c09f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# podmiana wag na konkretne wartości - tylko w celu testu poprawności implementacji w assercie poniżej\n",
    "np.random.seed(0)\n",
    "weight_sizes = [(4, 32), (32,), (32, 32), (32,), (32, 2), (2,)]\n",
    "new_weights = [np.random.normal(size=size) for size in weight_sizes]\n",
    "model.set_weights(new_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e46377-794b-44f1-9e7a-543e31c576d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "npt.assert_allclose(model.predict(state, verbose=0), np.array([[-3.4027915, -6.6636534]]),\n",
    "                    rtol=1e-5, atol=1e-8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6fdf58-2dfd-42e7-82ef-142d35ca4ee7",
   "metadata": {},
   "source": [
    "#### 3. Wybór akcji\n",
    "Zaimplementuj funkcję get_action, realizującą strategię epsilon-zachłanną, która z prawdopodopieństwem epsilon wybiera losową akcję, a z prawdopodobieństwem 1 - epsilon akcję o największej Q-wartości dla danego stanu \n",
    "* Do losowania wykorzystać funkcję np.random.uniform()\n",
    "* Do wyboru akcji wywołać predykcję sieci dla danego stanu (model.predict(...)) i wykorzystać funkcję argmax\n",
    "* W funkcji predict zastosować argument verbose=0, który spowoduje, że nie będzie wydruków"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5481ae6e-486e-43b5-a993-e382a883f3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_action(env, state, model, epsilon):\n",
    "    #TODO\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd26505-a25b-4e3d-87d6-b16044a6d030",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tu powinin być losowy wynik - mozna sobie uruchomić kilka razy\n",
    "get_action(env, state, model, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f58561-8278-4e00-ae0b-331498259d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a tu predykcja sieci\n",
    "assert(get_action(env, state, model, 0) == 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad6a873-16bb-4965-9c90-69ee38f23a5e",
   "metadata": {},
   "source": [
    "#### 4. Aktualizacja wag modelu\n",
    "Zaimplementować funkcję aktualizującą wagi modelu:\n",
    "\n",
    "1. Wybierz batch losowych próbek z bufora\n",
    "2. Oblicz wartość $y$ ($y$ - oczekiwane wyjścia sieci, do których będziemy douczać):\n",
    "  \n",
    "    $$y_a = \\begin{cases} r & \\text{jeśli to koniec epizodu} \\\\ r + \\gamma \\max_{a'} Q(s', a') &      \\text{w przeciwnym przypadku} \\end{cases}$$\n",
    "Wskazówka: zamiast if można wykorzystać w drugim członie równania mnożenie przez (1-dones).\n",
    "\n",
    "Chcemy aktualizaować $Q(s, a)$ tylko dla akcji $a$, która faktycznie została wykonana. Nasza sieć ma dwa wyjścia (dla dwóch akcji), więc do funkcji fit zawsze musimy podawać jako $y$ macierz o rozmiarze rozmiar batcha $\\times$ liczba możliwych akcji.\n",
    "\n",
    "Aby nie aktualizować wyjscia sieci dla drugiej (niewykonanej) akcji realizujemy punkt 2 następująco:\n",
    "\n",
    "* wyliczamy wartość $y_a$ wg wzoru powyżej (wymiar rozmiar batcha)\n",
    "* wyliczmy $y$ jako predykcję sieci dla $s$ (wymiar rozmiar batcha $\\times$ liczba akcji)\n",
    "* podmieniamy wartość $y$ na $y_a$ dla akcji $a$\n",
    "* w ten sposób wartość $y$ dla niewybranej akcji jest taka sama jak predykcja sieci, więc błąd jest 0, więc nie aktualizujemy\n",
    "\n",
    "Uwaga: w tym ćwiczeniu nie korzystamy z Q-tabeli, aproksymujemy Q-wartości z wykorzystaniem sieci neuronowej.\n",
    "\n",
    "3. Zaktualizuj wagi sieci neuronowej\n",
    "\n",
    "Bufor jest listą krotek zawierających (state, action, reward, new_state, done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeccdf5e-7b11-4dc2-bcb0-a35f55ebe519",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = np.array([[-0.005, 0.001, 0.0001, 0.0001]])\n",
    "new_state = np.array([[0.005, 0.001, 0.0001, 0.0001]])\n",
    "buffer = [(state, 0, 1, new_state, True),\n",
    "          (state, 1, 1, new_state, False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e479238b-4125-4b0b-906b-8a5b724e433a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(model, buffer, batch_size):\n",
    "    # TODO jeśli rozmiar bufora jest mniejszy niż batch_size wyjdź z funkcji poprzez return\n",
    "\n",
    "    minibatch = # TODO wybierz buffer_size losowych próbek z bufora (funkcja samle z modułu random)\n",
    "\n",
    "    # wyciągamy z bufora odpowienie dane i zamieniamy na macierze numpy\n",
    "    states, actions, rewards, new_states, dones = zip(*minibatch)\n",
    "    actions, rewards, dones = np.array(actions), np.array(rewards), np.array(dones)\n",
    "    # states to krotka (o batch_size elementach) np.arrayów o rozmiarze 1 x state size (new states też)\n",
    "    states = # TODO zamień na np.array i zmień rozmiar na batch_size x state_size\n",
    "    new_states = # TODO zamień na np.array i zmień rozmiar na batch_size x state_size\n",
    "\n",
    "    y_a = # wyznacz y_a według wzoru w 2\n",
    "    y = # wyznacz y jako predykcję sieci dla states\n",
    "        \n",
    "    for i, action in enumerate(actions):\n",
    "        # TODO podmień wartość y dla odpowiedniej akcji na y_a\n",
    "        \n",
    "    # ucz model przez jedną epokę, zastosuj verbose=0, aby uniknąć wydruków"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2099b11c-532e-4998-96e1-86a8d92ee537",
   "metadata": {},
   "outputs": [],
   "source": [
    "update(model, buffer, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcedec3f-26f5-4cea-8530-186bbb6ef59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "npt.assert_allclose(model.layers[2].get_weights()[1], np.array([-0.08411561, -0.563301]),\n",
    "                    rtol=1e-5, atol=1e-8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde7c6e5-2432-4c65-9400-d0b9ff370fb0",
   "metadata": {},
   "source": [
    "#### 5. Epizod z wykorzystaniem zachłannej polityki\n",
    "Zaimplementuj funkcję, która wykona jeden epizod, realizując politykę zachłanną (zawsze wybieramy akcję o najlepszej q-vartości) i zwróci wartość skumulowanej nagrody."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf96ed82-37e8-4c55-bd15-422b920a2009",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_eval_episode(env, model):\n",
    "    state, info = env.reset()\n",
    "    state = state.reshape(1, state_size)\n",
    "    done = False\n",
    "    rewards = 0\n",
    "    for j in range(max_steps):\n",
    "        q_values = #TODO predykcja sieci dla danego stanu\n",
    "        action = #TODO wybierz najlepszą akcję\n",
    "        state, reward, terminated, truncated, info = env.step(action)\n",
    "        state = state.reshape(1, state_size)\n",
    "        done = terminated or truncated\n",
    "        # TODO aktualizacja sumy nagród\n",
    "        if done:\n",
    "            break\n",
    "    env.close()\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fada48e8-d4a7-483d-897a-6dd9fbed113c",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_eval_episode(env, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763a25f2-18a8-421f-b9cb-001e241a541b",
   "metadata": {},
   "source": [
    "#### Algorytm uczenia\n",
    "Zaimplementuj pętlę algorymu uczenia:\n",
    "\n",
    "1. Zainicjalizuj sieć neuronową i pusty bufor próbek.\n",
    "2. Z prawdopodobieństwem $\\epsilon$ wybierz losową akcję, w przeciwnym przypadku wybierz akcję zgodnie z polityką\n",
    "3. Poprzez interakcje ze środowiskiem uzyskaj $(s, a, r, s', done)$\n",
    "4. Dodaj $(s, a, r, s', done)$ do bufora\n",
    "5. Zaktualizuj wagi sieci neuronowej\n",
    "6. Zaktualizuj $\\epsilon$ - jeśli jest większy niż epsilon_min, pomnóż przez epsilon_decay\n",
    "7. Dopóki nie warunek stopu idź do 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef89557f-0e8d-46d8-bd71-7f3bc0b734c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_size = 4\n",
    "epsilon = 1\n",
    "epsilon_min = 0.01\n",
    "epsilon_decay = 0.99\n",
    "gamma = 0.95\n",
    "batch_size = 32\n",
    "buffer_size = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca91e6d1-c3a7-48f9-8cfe-3d0a16529b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer = []\n",
    "model = make_network()\n",
    "for i in range(200):\n",
    "    state, info = env.reset()\n",
    "    state = state.reshape(1, state_size)\n",
    "    done = False\n",
    "    rewards = 0\n",
    "    for j in range(max_steps):\n",
    "        action = # TODO wybierz akcję stosując strategię epsilon zachłanną\n",
    "        new_state, reward, terminated, truncated, info = # TODO wykonaj akcję\n",
    "        new_state = new_state.reshape(1, state_size)\n",
    "        done = terminated or truncated\n",
    "        rewards += reward\n",
    "        # na koniec zamianiamy nagrodę na -10, żeby zaznaczyć, że nie chcemy żeby wahadło spadało\n",
    "        if done:\n",
    "            reward = -10\n",
    "        # TODO dodaj uzyskaną próbkę do bufora\n",
    "        # TODO jesli długość bufora jest większa niż buffer_size usuń pierwszą próbkę (użyć  list.pop([i]))\n",
    "        state = # TODO zaktualizuj stan\n",
    "        # TODO zaktualizuj wagi modelu\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "    env.close()\n",
    "    # epsilon decay\n",
    "    # TODO zaktualizuj epsilon zgodnie z 6\n",
    "    print(i, \": \", rewards)\n",
    "    \n",
    "    if i % 10 == 0:\n",
    "        eval_reward = # TODO wyznacz nagrodę uzyskaną z wykorzystaniem startegii optymalnej\n",
    "        print(\"epsilon = {:.2f}\".format(epsilon), \"eval run reward = \", eval_reward)\n",
    "        if eval_reward >= 200:\n",
    "            print(\"Rozwiązane!\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0f7ac5-19cd-490e-bb8d-ec52457aaf90",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_vid = RecordVideo(env, video_folder=\"videos\", name_prefix=\"agent-control\", disable_logger=True)\n",
    "run_eval_episode(env_vid, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75a262c-1cab-4fd0-93ca-70158778e434",
   "metadata": {},
   "source": [
    "### Pytania\n",
    "1. Jak oceniasz na podstawie przygotowanego nagrania jakość uzyskanego rozwiązania?\n",
    "2. Jeśli są jakieś problemy to jak je można poprawić?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92e8e36-8c3d-45e0-aa6d-878cb7187bdc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
