{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2 Sieci Neuronowe w Automatyce -  algorytm wstecznej propagacji błędów z biblioteką NumPy\n",
    "\n",
    "Imię i nazwisko: ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wzory na podstawie: https://www.deeplearning.ai/deep-learning-specialization/ Neural Networks and Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Punktacja (do 10 pkt.)\n",
    "* Poprawnie wypełniony notebook 6 pkt.\n",
    "* Odpowiedzi na pytania na końcu 3 pkt. (1 pytanie = 1 punkt)\n",
    "* Bonusy dla chętnych do 2 pkt.\n",
    "* Suma z laboratorium nie może wynosić więcej niż 10 pkt. (wyniki większe niż 10 są obcinane do 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt # biblioteka do wykresów\n",
    "from mpl_toolkits.mplot3d import Axes3D # do wykresu 3D\n",
    "\n",
    "from sklearn.datasets import make_classification # generacja danych, sklearn - bardzo przydatna biblioteka do uczenia maszynowego"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generacja danych\n",
    "\n",
    "Problem klasyfikacji binarnej (kropka żółta czy fioletowa?) dla trzech zmiennych wejściowych $x_1$, $x_2$, $x_3$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 300\n",
    "data = make_classification(n_samples=m, n_features=3, n_informative=3, n_redundant=0, n_classes=2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data[0]\n",
    "Y = data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "cax = ax.scatter(X[:,0], X[:,1], X[:,2], c=Y.transpose())\n",
    "ax.set_xlabel('$x_1$')\n",
    "ax.set_ylabel('$x_2$')\n",
    "ax.set_zlabel('$x_3$')\n",
    "fig.colorbar(cax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sieć neuronowa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe src=\"https://drive.google.com/file/d/1s3mGcho6uv2VdMb4CBsw686qwhpp2mRw/preview\" width=\"640\" height=\"480\"></iframe>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<iframe src=\"https://drive.google.com/file/d/1s3mGcho6uv2VdMb4CBsw686qwhpp2mRw/preview\" width=\"640\" height=\"480\"></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \n",
    "\\begin{bmatrix}\n",
    "w_{11}^{[1]} & w_{12}^{[1]}  & w_{13}^{[1]}\\\\\n",
    "\\vdots & \\vdots & \\vdots \\\\\n",
    "\\vdots & \\vdots & \\vdots \\\\\n",
    "w_{41}^{[1]} & w_{42}^{[1]}  & w_{43}^{[1]}\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "x_1 \\\\\n",
    "x_2 \\\\\n",
    "x_3\n",
    "\\end{bmatrix}\n",
    "+\n",
    "\\begin{bmatrix}\n",
    "b_1^{[1]} \\\\\n",
    "b_2^{[1]} \\\\\n",
    "b_3^{[1]} \\\\\n",
    "b_4^{[1]}\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "z_1^{[1]} \\\\\n",
    "z_2^{[1]} \\\\\n",
    "z_3^{[1]} \\\\\n",
    "z_4^{[1]}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$ z^{[1]} = W^{[1]}x + b^{[1]} $$\n",
    "\n",
    "$$ a^{[1]} = \\varphi^{[1]}(z^{[1]}) $$\n",
    "\n",
    "$$ z^{[2]}  = W^{[2]}a^{[1]} + b^{[2]} $$\n",
    "\n",
    "$$ a^{[2]} = \\varphi^{[2]}(z^{[2]}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Incjalizacja wartości wag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize():\n",
    "    # wartości W losujemy z rozkładu normalnego o średniej 0 i odchyleniu standardowym 0.1, wartości b ustawiamy na 0\n",
    "    W1 = np.random.normal(0, 0.1, size=(4, 3))\n",
    "    b1 = np.zeros(shape=(4, 1))\n",
    "    W2 = # TODO\n",
    "    b2 = # TODO\n",
    "    return W1, b1, W2, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1, b1, W2, b2 = initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('W1 = ', W1)\n",
    "print('b1 = ', b1)\n",
    "print('W2 = ', W2)\n",
    "print('b2 = ', b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(W1.shape == (4, 3))\n",
    "assert(b1.shape == (4, 1))\n",
    "assert(W2.shape == (1, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.testing.assert_almost_equal(W2, [[0.07610377, 0.0121675, 0.04438632, 0.03336743]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funkcje aktywacji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    # sigmoidalna funkcja aktywacji dla macierzy Z\n",
    "    return # TODO\n",
    "\n",
    "def leaky_relu(Z):\n",
    "    # funkcja aktywacji leaky relu dla macierzy Z (leaky_relu(Z) = max(0.01 * Z, Z))\n",
    "    return # TODO\n",
    "\n",
    "def leaky_relu_grad(Z):\n",
    "    # pochodna funkcji leaky relu dla macierzy Z\n",
    "    # https://numpy.org/doc/stable/reference/generated/numpy.where.html\n",
    "    return # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testZ = np.array([[-3, -2, -1], [0, 1, 2]])\n",
    "print('Sigmoid:')\n",
    "print(sigmoid(testZ))\n",
    "print('Leaky ReLu:')\n",
    "print(leaky_relu(testZ))\n",
    "print('Leaky ReLu pochodna:')\n",
    "print(leaky_relu_grad(testZ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.testing.assert_almost_equal(sigmoid(testZ), [[0.04742587, 0.11920292, 0.26894142], [0.5, 0.73105858, 0.88079708]])\n",
    "np.testing.assert_almost_equal(leaky_relu(testZ), [[-0.03, -0.02, -0.01], [0, 1, 2]])\n",
    "np.testing.assert_almost_equal(leaky_relu_grad(testZ), [[0.01, 0.01, 0.01], [1., 1., 1.]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Propagacja w przód\n",
    "\n",
    "Wektoryzacja dla wielu przykładów\n",
    "\n",
    "$$ X = \\begin{bmatrix}\n",
    "x_1 ^{(1)} & x_1 ^{(2)} & \\dots & x_1 ^{(m)}\\\\\n",
    "x_2 ^{(1)} & x_2 ^{(2)} & \\dots & x_2 ^{(m)}\\\\\n",
    "x_3 ^{(1)} & x_3 ^{(2)} & \\dots & x_3 ^{(m)}\n",
    "\\end{bmatrix} $$\n",
    "\n",
    "$$ Z^{[1]} =  W^{[1]}X + b^{[1]}  $$\n",
    "$$ A^{[1]} = \\varphi^{[1]}(Z^{[1]}) $$\n",
    "$$ Z^{[2]} = W^{[2]}A^{[1]} + b^{[2]}  $$\n",
    "$$ A^{[2]} = \\varphi^{[2]}(Z^{[2]}) $$\n",
    "$$ \\hat{Y} = A^{[2]} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xt = X.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = Y.reshape(1, m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(X, W1, W2, b1, b2):\n",
    "    # wyznaczamy wyjście sieci i wartości w warstwach ukrytych\n",
    "    Z1 = np.dot(W1, X) + b1\n",
    "    A1 = # TODO\n",
    "    Z2 = # TODO\n",
    "    A2 = # TODO\n",
    "    Y_hat = # TODO\n",
    "    return Z1, A1, Z2, A2, Y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z1, A1, Z2, A2, Y_hat = forward(Xt, W1, W2, b1, b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(Z1.shape == (4, m))\n",
    "assert(Z2.shape == (1, m))\n",
    "assert(Y_hat.shape == (1, m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.testing.assert_almost_equal(Y_hat[0][:3], [0.50025724, 0.49987587, 0.50083385])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funkcja kosztu\n",
    "\n",
    "Dla klasyfikacji binarnej stosujemy entropię krzyżową\n",
    "$$ -\\frac{1}{m} \\sum_{i=1}^m \\left( y_i \\log(\\hat{y}_i) + (1 - y_i)\\log(1-\\hat{y}_i)\\right) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def J(Y, Y_hat):\n",
    "    # binarna entropia krzyżowa\n",
    "    # proszę zastosować logarytm naturalny (np.log)\n",
    "    return # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(round(J(np.array([[1, 0, 1]]), np.array([[0.98, 0.1, 0.5]])), 4) == 0.2729)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wyznaczanie pochodnych metodą wstecznej propagacji błędów\n",
    "\n",
    "$$ \\frac{\\partial J}{\\partial Z^{[2]}} = A^{[2]} - Y$$\n",
    "$$ \\frac{\\partial J}{\\partial W^{[2]}} =\\frac{1}{m} \\frac{dJ}{\\partial Z^{[2]}} A^{[1]T} $$\n",
    "$$ \\frac{\\partial J}{\\partial b^{[2]}} = \\frac{1}{m} np.sum(\\frac{dJ}{\\partial Z^{[2]}}, axis = 1, keepdims=True) $$\n",
    "$$\\frac{dJ}{\\partial Z^{[1]}} = W^{[2]T}\\frac{dJ}{\\partial Z^{[2]}} * \\varphi^{[1]{\\prime}}(z_1)$$\n",
    "\n",
    "$$ \\frac{\\partial J}{\\partial W^{[1]}} = \\frac{dJ}{\\partial Z^{[1]}} X^T $$\n",
    "$$ \\frac{\\partial J}{\\partial b^{[1]}} = \\frac{1}{m} np.sum(\\frac{dJ}{\\partial Z^{[1]}}, axis = 1, keepdims=True) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_gradients(Y, A2, A1, Z1, W2, W1, b2, b1, X, m):\n",
    "    dZ2 = A2 - Y\n",
    "    dW2 = # TODO\n",
    "    db2 = # TODO\n",
    "    dZ1 = # TODO\n",
    "    dW1 = # TODO\n",
    "    db1 = # TODO\n",
    "    return dW2, db2, dW1, db1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dW2, db2, dW1, db1 = calculate_gradients(Y, A2, A1, Z1, W2, W1, b2, b1, Xt, m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.testing.assert_almost_equal(dW2, [[-0.02540348, -0.03184805, -0.00094008, -0.01338454]])\n",
    "np.testing.assert_almost_equal(db2, [[0.00116861]])\n",
    "np.testing.assert_almost_equal(dW1, [[-2.12742532, -1.1236145 , -1.63209061], [-0.26206997, -0.40640333, -0.18799057],\n",
    "                                     [-0.75340629, -2.36955757, -2.24738247],[-1.57469511,  0.94111372, -0.56991805]])\n",
    "np.testing.assert_almost_equal(db1, [[-0.00156242], [0.00022612], [-0.00021417], [-0.00262392]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorytm największego spadku gradientu\n",
    "\n",
    "1. Przypisz wagom losowe wartości początkowe\n",
    "2. Wyznacz wyjście sieci $\\hat{Y}$\n",
    "3. Wyznacz wartość funkcji kosztu $J$ \n",
    "4. $$ dW = \\frac{\\partial J}{\\partial W} \\qquad db = \\frac{\\partial J}{\\partial w}$$\n",
    "$$ W = W - \\alpha dW $$\n",
    "$$ b = b - \\alpha db $$\n",
    "5. dopóki nie warunek stopu idź do 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(W2, W1, b2, b1, dW2, dW1, db2, db1, alpha=0.001):\n",
    "    W2 = W2 - alpha * dW2\n",
    "    W1 = # TODO\n",
    "    b2 = # TODO\n",
    "    b1 = # TODO\n",
    "    return W2, W1, b2, b1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W2, W1, b2, b1 = update(W2, W1, b2, b1, dW2, dW1, db2, db1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(W2.shape == (1, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dokładność\n",
    "\n",
    "Wyjściem sieci jest liczba z zakresu 0-1. Dla $\\hat{y}^{(i)} > 0.5$ przewidujemy 1 dla pozostałych 0.\n",
    "Dokładność (accuracy) jest procentem przykładów, które zostały przewidziane przez sieć prawidłowo (dla których $\\hat{y}^{(i)} = y^{(i)}$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(Y, Y_hat):\n",
    "    return # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(accuracy(np.array([[1, 0, 1, 0]]), np.array([[0.98, 0.1, 0.51, 0.7]])) == 75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I w końcu - uczenie sieci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inicjalizacja wag\n",
    "W1, b1, W2, b2 = initialize()\n",
    "# lista do zapisu wartości funkcji kosztu w kolejnych krokach uczenia (na początku pusta)\n",
    "J_history = []\n",
    "# lista do zapisu dokładności w kolejnych krokach (na początku pusta)\n",
    "acc_history =[]\n",
    "for i in range(100000):\n",
    "    Z1, A1, Z2, A2, Y_hat = forward(Xt, W1, W2, b1, b2)\n",
    "    J_history.append(J(Y, Y_hat))\n",
    "    acc_history.append(accuracy(Y, Y_hat))\n",
    "    dW2, db2, dW1, db1 = calculate_gradients(Y, A2, A1, Z1, W2, W1, b2, b1, Xt, m)\n",
    "    W2, W1, b2, b1 = update(W2, W1, b2, b1, dW2, dW1, db2, db1, alpha=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(J_history)\n",
    "plt.title('Funkcja kosztu w zależności od iteracji')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(acc_history)\n",
    "plt.title('Dokładność w zależności od iteracji')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dokładność na koniec\n",
    "acc_history[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predykcja dla nowych danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# punkt do sprawdzenia\n",
    "x_test1 = np.array([[3, 3, 0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nowy punkt zaznaczony czerwoną kropką\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "cax = ax.scatter(X[:,0], X[:,1], X[:,2], c=Y.reshape(m,))\n",
    "ax.scatter(x_test1[0][0], x_test1[0][1], x_test1[0][2], c='r')\n",
    "ax.set_xlabel('$x_1$')\n",
    "ax.set_ylabel('$x_2$')\n",
    "ax.set_zlabel('$x_3$')\n",
    "fig.colorbar(cax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predykacja bliska 1 oznacza żółtą kropkę, bliska 0 fioletową kropkę\n",
    "Z1, A1, Z2, A2, Y_hat = forward(x_test1.transpose(), W1, W2, b1, b2)\n",
    "print(Y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytania\n",
    "1. Jak wpływa wartość $\\alpha$ na przebieg uczenia? (proszę spróbować różnych wartości)\n",
    "2. Czy przy takiej samej wartości $\\alpha$ zawsze dostajemy taki sam wynik? Dlaczego?\n",
    "3. Czy uzyskane wyniki pozwalają stwierdzić, że sieć działa dobrze?\n",
    "\n",
    "Tu proszę wpisać swoje odpowiedzi:\n",
    "1. ...\n",
    "2. ...\n",
    "3. ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dla chętnych\n",
    "(proszę sobie zapisać notebook pod nową nazwą do eksperymentów i pozmieniać wybrane)\n",
    "1. (łatwe) Inna funkcja aktywcji warstwy ukrytej\n",
    "2. (łatwe) Zmiana liczby neuronów w warstwie ukrytej\n",
    "3. (średnie) Dodanie kolejnej warstwy ukrytej\n",
    "4. (trudne - trzeba przeliczyć wzory) Klasyfikacja dla Y o trzech możliwych wartościach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
